{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU notes and implementation\n",
    "\n",
    "**This notebook demostrates the basic idea in [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf). The efficiency of the algorithm is not in first consideration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas in section *2.1 Modified n-gram precision*\n",
    "> We formalize this intuition as the modified unigram precision. To compute this, one first counts the maximum number of times a word occurs in any **single reference translation**. Next, one clips the total count of each candidate word by its maximum reference count, adds these clipped counts up, and divides by the total (unclipped) number of candidate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _mostOccurSentenceOfAWord(word, references):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the sentence which word occurs most and how many time this word occurs in this sentence\n",
    "    Inputs:\n",
    "        word: word -- a single word in candidate sentence\n",
    "        references: list of sentence -- references sentences\n",
    "    Returns:\n",
    "        mostOccurs: sentence -- the sentence which the word occurs most times.\n",
    "        mostOccursTimes: integer -- how many times does word occur in mostOccurs\n",
    "    \"\"\"\n",
    "    counts = list(map(lambda reference: reference.count(word), references))\n",
    "    def argmax(L):\n",
    "        return np.argmax(np.array(L)) # np.argmax will return the first one index\n",
    "    mostOccursIndex = argmax(counts)\n",
    "    mostOccurs = references[mostOccursIndex]\n",
    "    mostOccursTimes = counts[mostOccursIndex]\n",
    "    return mostOccurs, mostOccursTimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Count_{clip}=min(Count, Max\\_Ref\\_Count)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modifiedUnigramPrecision(candidate, references, returnCounts=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the modified unigram precision, or (count_clip, numCandidateUniGrams) pair\n",
    "    Inputs:\n",
    "        candidate: sentence -- candidate sentence\n",
    "        references: list of sentence -- references sentences\n",
    "        returnCounts: boolean -- False to return precision, True to return counts\n",
    "    Returns:\n",
    "        modifiedUnigramPrecision: float -- modified unigram precision defined in paper\n",
    "        OR\n",
    "        (count_clip, numCandidateUniGrams):\n",
    "            countClip: int -- count_clip in paper\n",
    "            numCandidateUniGrams: int -- number of candidate unigrams\n",
    "    \"\"\"\n",
    "    countClip = 0\n",
    "    for word in set(candidate):\n",
    "        _, mostOccursTimes = _mostOccurSentenceOfAWord(word, references)\n",
    "        countClip += min(candidate.count(word), mostOccursTimes)\n",
    "    numCandidateUniGrams = len(candidate)\n",
    "    return  (countClip, numCandidateUniGrams) if returnCounts else (countClip/numCandidateUniGrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Modified n-gram precision is computed similarly for any n: all candidate n-gram counts and their\n",
    "corresponding maximum reference counts are collected. The candidate counts are clipped by their\n",
    "corresponding reference maximum value, summed, and divided by the total number of candidate n-grams.\n",
    "\n",
    "Treat n-gram as a single word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modifiedNGramPrecision(n, candidate, references, returnCounts=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the modified n-gram precision or (count_clip, numCandidateNGrams) pair using modifiedUnigramPrecision()\n",
    "    Inputs:\n",
    "        n: int -- n for n-gram\n",
    "        candidate: sentence -- candidate sentence\n",
    "        references: list of sentence -- references sentences\n",
    "    Returns:\n",
    "        modifiedNGramPrecision: float -- modified n-gram precision defined in paper\n",
    "        OR\n",
    "        (count_clip, numCandidateNGrams):\n",
    "            countClip: int -- count_clip in paper\n",
    "            numCandidateNGrams: int -- number of candidate n-grams\n",
    "    \"\"\"    \n",
    "    def sentenceToNGrams(n, sentence):\n",
    "        nGrams = []\n",
    "        for idx in range(len(sentence)-n+1):\n",
    "            ngram = \" \".join(sentence[idx:idx+n])\n",
    "            nGrams.append(ngram)\n",
    "        return nGrams\n",
    "    \n",
    "    candidateGrams = sentenceToNGrams(n, candidate)\n",
    "    referencesGrams = list(map(lambda ref: sentenceToNGrams(n, ref), references))\n",
    "    countClip, numCandidateNGrams = modifiedUnigramPrecision(candidateGrams, referencesGrams, returnCounts=True)\n",
    "    return (countClip, numCandidateNGrams) if returnCounts else (countClip/numCandidateNGrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidate1 = \"It is a guide to action which ensures that the military always obeys the commands of the party\".split()\n",
    "candidate2 = \"It is to insure the troops forever hearing the activity guidebook that party direct\".split()\n",
    "references = [\"It is a guide to action that ensures that the military will forever heed Party commands\".split(),\n",
    "            \"It is the guiding principle which guarantees the military forces always being under the command of the Party\".split(),\n",
    "            \"It is the practical guide for the army always to heed the directions of the party\".split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test modifiedUnigramPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifiedUnigramPrecision(candidate1, references) #  Section 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifiedUnigramPrecision(candidate2, references) #  Section 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifiedUnigramPrecision(references[0], references) #  Section 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifiedUnigramPrecision(references[1], references) #  Section 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifiedUnigramPrecision(references[2], references) #  Section 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test modifiedNGramPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5882352941176471"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifiedNGramPrecision(2, candidate1, references) #  Section 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5882352941176471"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifiedNGramPrecision(2, candidate2, references) #  Section 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideas in section *2.1.1 Modiﬁed n-gram precision on blocks of text*\n",
    "\n",
    "> We first compute the n-gram matches sentence by sentence. Next, we add the clipped n-gram\n",
    "counts for all the candidate sentences and divide by the number of candidate n-grams in the test corpus to compute a modiﬁed precision score, $p_n$, for the entire test corpus.\n",
    "\n",
    "\n",
    "$$ p_n = \\frac{\\sum_{\\mathcal{C}\\in{Candidates}} \\sum_{n-gram\\in\\mathcal{C}} Count_{clip}(n-gram)}\n",
    "          {\\sum_{\\mathcal{C}'\\in{Candidates}} \\sum_{n-gram'\\in\\mathcal{C}'} Count(n-gram')} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modifiedPrecisionScore(n, candidates, referencesList):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns p_n in paper.\n",
    "    Inputs:\n",
    "        n: int -- n for n-gram\n",
    "        candidates: list of sentence -- candidate sentences\n",
    "        referencesList: list of list of sentence -- list of references sentences\n",
    "    Returns:\n",
    "        p_n: float -- p_n in paper\n",
    "    \"\"\"   \n",
    "    totalCountClip = 0\n",
    "    totalCount = 0\n",
    "    for idx in range(len(candidates)):\n",
    "        CountClip, Count = modifiedNGramPrecision(n, candidates[idx], referencesList[idx], returnCounts=True)\n",
    "        totalCountClip += CountClip\n",
    "        totalCount += Count\n",
    "    p_n = totalCountClip/totalCount\n",
    "    return p_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates=[candidate2, candidate1]\n",
    "referencesList = [references, references]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36666666666666664"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifiedPrecisionScore(2, candidates, referencesList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideas in section *2.1.3 Combining the modiﬁed n-gram precisions*\n",
    "BLEU uses the average logarithm with uniform weights, which is equivalent to using the geometric mean of the modiﬁed n-gram precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 9 artists>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQcZdkhpk3CWpQcZdkhr0/0cqHI9q1xuQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x204ce068240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxGram = 9\n",
    "plt.bar(range(maxGram), np.array([modifiedPrecisionScore(i, candidates, referencesList) for i in range(maxGram)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ideas in section *2.2.2 Sentence brevity penalty*\n",
    "\n",
    "Candidate translations longer than their references are already penalized by the modified n-gram precision measure: there is no need to penalize them again.  \n",
    "\n",
    " We call the closest reference sentence length the **“best match length.”**  \n",
    " \n",
    "If we computed the brevity penalty sentence by sentence and averaged the penalties, then length deviations on short sentences would be punished harshly.  \n",
    "\n",
    "\n",
    "Instead, we compute the brevity penalty over the **entire corpus** to allow some freedom at the sentence level. We first compute the test corpus’ *effective reference length*, $$r$$, by summing the *best match lengths* for each candidate sentence in the **corpus**.  \n",
    "\n",
    " We choose the brevity penalty to be a decaying exponential in $$\\frac{r}{c}$$, where $c$ is the total length of the candidate translation corpus.  \n",
    " \n",
    "We first compute the geometric average of the modified n-gram precisions, $p_n$, using n-grams up to length $N$ and positive weights $w_n$ summing to one. Next, we compute the brevity penalty BP,\n",
    "$$BP = 1 \\quad \\quad \\quad if c>r \\\\ \\quad \\quad \\quad e^{(1-r/c)} \\quad if c\\le r\n",
    "$$\n",
    "Then,\n",
    "$$BLEU=BP\\cdot \\exp(\\sum^N_{n=1}\\omega_n \\log p_n)$$\n",
    "OR\n",
    "$$ log BLEU = min(1-\\frac{r}{c}, 0) + \\sum^N_{n=1} \\omega _n log p_n$$\n",
    "In our baseline, we use $N = 4$ and uniform weights $\\omega_n = 1/N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BLEU(n, candidates, referencesList):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns BLEU in paper, with uniform weight \\omega_n=1/N.\n",
    "    Inputs:\n",
    "        n: int -- n for max n-gram\n",
    "        candidates: list of sentence -- candidate sentences\n",
    "        referencesList: list of list of sentence -- list of references sentences\n",
    "    Returns:\n",
    "        BLEU: float -- BLEU score\n",
    "    \"\"\"   \n",
    "    def bestMatchLength(candLen, refLens):\n",
    "        matchLen = list(map(lambda refLen: abs(candLen-refLen), refLens))\n",
    "        return refLens[np.argmin(matchLen)]\n",
    "    \n",
    "    candLens = [len(candidate) for candidate in candidates]\n",
    "    refLenLists = [[len(reference) for reference in references] for references in referencesList]\n",
    "    \n",
    "    bestMatchLengths = [bestMatchLength(candLens[idx], refLenLists[idx]) for idx in range(len(candidates))]\n",
    "    r = sum(bestMatchLengths)     \n",
    "    c = sum(candLens)\n",
    "\n",
    "    if c>r:\n",
    "        BP = 1\n",
    "    else:\n",
    "        BP = np.exp(1-(r/c))\n",
    "    \n",
    "    P_n = [modifiedPrecisionScore(i, candidates, referencesList) for i in range(1, n+1)]\n",
    "    logP_n = list(map(lambda x: np.log(x), P_n)) # occasionally encounter log(0) when test short candidates with big N\n",
    "    geometricMean = np.exp(sum(logP_n)/n)\n",
    "    BLEU = BP*geometricMean\n",
    "    return BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Scores should grow monotonously.  \n",
    "In the paper candidate1 is a better translation than candidate2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates=[candidate2, candidate2]\n",
    "referencesList = [references, references]\n",
    "\n",
    "BLEU(4, candidates, referencesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3043537261305561"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates=[candidate2, candidate1]\n",
    "referencesList = [references, references]\n",
    "\n",
    "BLEU(4, candidates, referencesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3043537261305561"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates=[candidate1, candidate2]\n",
    "referencesList = [references, references]\n",
    "\n",
    "BLEU(4, candidates, referencesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5045666840058485"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates=[candidate1, candidate1]\n",
    "referencesList = [references, references]\n",
    "\n",
    "BLEU(4, candidates, referencesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7534483667732512"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates=[candidate1, references[0]]\n",
    "referencesList = [references, references]\n",
    "\n",
    "BLEU(4, candidates, referencesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates=[references[0], references[0]]\n",
    "referencesList = [references, references]\n",
    "\n",
    "BLEU(4, candidates, referencesList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
